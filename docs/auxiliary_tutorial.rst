Clustering and visualizing data
================================

You can also use random features generated by xGPR for kernel PCA and for
clustering. xGPR has a built-in tool for kernel PCA.
For kernel k-means, we have a tool that will generate the random features
for your data for you, and then you just run k-means on the resulting features
using your package of choice. This is (approximately) equivalent to running
kernel k-means on the original data, but much faster, because there is no
need to construct an N x N kernel matrix! Also, it enables you to use
the graph and sequence kernels in xGPR for clustering or visualization.
(It is also possible to use another algorithm aside from k-means to cluster
the random features representations, although due to its scalability k-means
is a popular choice.)

In a future version of xGPR, we may add our own implementation of k-means.
For now, though, we don't provide one, which means you'll have to use
one from another package. Unfortunately, many such implementations
(e.g. scikit-learn) require you to have all the data loaded into memory.
This sets an upper bound on how many random features you can generate
in order to do clustering (also, k-means with a large number of
random features is likely to be very slow).

Two possible alternatives are to generate a smaller number of random
features (e.g. 512 - 1024) or to use kPCA to get the top say 100
principal components from a larger number of random features. The
kernel approximation will not be as accurate as if we used more
random features, but it will still be good enough for many clustering
purposes.


Visualizing with kPCA
---------------------------------

Here is a kernel PCA of the QM9 dataset (130,000 small
molecules) generated using the random features produced by a GraphConv1d
kernel, trained on one-hot encoded input data:

.. image:: images/kernel_pca.png
   :width: 350
   :alt: Kernel PCA


In this case, you can see that the label we want to predict correlates quite
well with the first principal component. This will not always be true, of course,
because using only two principal components is of course throwing away a
substantial amount of information (as with any other 2d visualization tool).

Here's how to do approximate kPCA in xGPR:::

  from xGPR import KernelxPCA

  kernel_xpca = KernelxPCA(num_rffs = 1024, hyperparams = np.array([1.0]),
                    dataset = my_dataset, n_components = 2, kernel_choice = "RBF",
                    kernel_specific_params = {}, random_seed = 123, verbose = True)

  my_pca_transformed_data = kernel_xpca.predict(my_input_numpy_array,
                                           chunk_size = 2000)

Note some important things here. First, the ``KernelxPCA`` is like a greatly slimmed-
down xGPRegression model that doesn't do hyperparameter tuning and is fitted
as soon as it's created (for a large dataset and/or num_rffs, this may take a minute.)
This means that you have to supply (the log of) the
hyperparameters as a numpy array. All xGPR kernels have at least two hyperparameters,
and all of them share two hyperparameters -- you do *not* need to supply these. You
only need to supply the kernel-specific hyperparameters -- for the RBF, Matern,
GraphRBF, FHTConv1d etc. kernels, there is just one kernel-specific hyperparameter,
the lengthscale, as illustrated here. Some kernels (e.g. GraphPoly, Poly, Linear etc.)
do not have any kernel-specific hyperparameters so you do not need to supply anything
for this argument, it will be ignored.

How to choose a hyperparameter? If you've fitted a Gaussian process regression model
to your data, that's easy -- just call:::

  my_model.get_hyperparams()

the log of the hyperparameters is returned, you can plug it into kernel PCA. If NOT,
however, selecting a good hyperparameter can be a little trickier. See the examples / 
tutorials for some examples of how this affects the results. Of course, some kernels
(e.g. Poly or GraphArcCosine) do not require a hyperparameter to be set, but these
make some additional assumptions (see :doc:`Kernel choices<kernel_info/kernel_list>`.

Setting a larger ``chunk_size`` increases the number of datapoints that
are processed at a time, which increases memory consumption but
slightly increases speed. The ``KernelxPCA`` object when created fits
an approximate kernel PCA, and then when ``predict`` is called, returns the
transformed inputs projected onto the (approximate) top ``n_components``
eigenvectors, so that ``my_pca_transformed_data`` will have shape
``(my_input_numpy_array.shape[0], n_components)``. For generating
a kernel PCA plot, just use 2 components; you could use more components
if you wanted to do clustering. To do k-means, take the resulting
PCA-transformed input data and run it through your favorite implementation
of kernel k-means.

K-means clustering
-------------------

To do approximate kernel k-means clustering, you just need to generate
random features for all of your datapoints then run a k-means clustering
algorithm (from another package) on the results. The tool you need to
do this is nearly identical to the kernel PCA tool above; the only difference
is it doesn't need to fit the data, and you don't need to supply a number
of components. Usage is like this:::

  from xGPR import KernelFGen

  fgen = KernelFGen(num_rffs = 512, hyperparams = np.array([1.0]),
                    dataset = my_dataset, kernel_choice = "RBF",
                    kernel_specific_params = {}, random_seed = 123, verbose = True)

  my_feature_rep = fgen.predict(my_input_numpy_array,
                                           chunk_size = 2000)

Now you can cluster ``my_feature_rep`` -- it's just a random features representation
of your input. See the section on kernel PCA above for more details on the options.
Also see the examples / tutorials section on the main page for some examples of how
to do kernel k-means for sequences or other kinds of data.
