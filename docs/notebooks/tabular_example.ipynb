{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1382a54",
   "metadata": {},
   "source": [
    "## Example: Fitting tabular data\n",
    "\n",
    "This straightforward warm-up example makes use of a small,\n",
    "fairly random UCI repository dataset with about 45,000 datapoints. We'll\n",
    "download this data, do some light preprocessing, and fit an RBF kernel.\n",
    "\n",
    "These experiments used xGPR 0.1.3.0 on a GTX1070 GPU with 8 GB of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1d0a6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "import wget\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from xGPR import xGPRegression as xGPReg\n",
    "from xGPR import build_online_dataset\n",
    "from xGPR import build_offline_fixed_vector_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39d7217e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 / unknown"
     ]
    }
   ],
   "source": [
    "fname = wget.download(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00265/CASP.csv\")\n",
    "raw_data = pd.read_csv(fname)\n",
    "os.remove(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec1e70f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSD</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.284</td>\n",
       "      <td>13558.30</td>\n",
       "      <td>4305.35</td>\n",
       "      <td>0.31754</td>\n",
       "      <td>162.1730</td>\n",
       "      <td>1.872791e+06</td>\n",
       "      <td>215.3590</td>\n",
       "      <td>4287.87</td>\n",
       "      <td>102</td>\n",
       "      <td>27.0302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.021</td>\n",
       "      <td>6191.96</td>\n",
       "      <td>1623.16</td>\n",
       "      <td>0.26213</td>\n",
       "      <td>53.3894</td>\n",
       "      <td>8.034467e+05</td>\n",
       "      <td>87.2024</td>\n",
       "      <td>3328.91</td>\n",
       "      <td>39</td>\n",
       "      <td>38.5468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.275</td>\n",
       "      <td>7725.98</td>\n",
       "      <td>1726.28</td>\n",
       "      <td>0.22343</td>\n",
       "      <td>67.2887</td>\n",
       "      <td>1.075648e+06</td>\n",
       "      <td>81.7913</td>\n",
       "      <td>2981.04</td>\n",
       "      <td>29</td>\n",
       "      <td>38.8119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.851</td>\n",
       "      <td>8424.58</td>\n",
       "      <td>2368.25</td>\n",
       "      <td>0.28111</td>\n",
       "      <td>67.8325</td>\n",
       "      <td>1.210472e+06</td>\n",
       "      <td>109.4390</td>\n",
       "      <td>3248.22</td>\n",
       "      <td>70</td>\n",
       "      <td>39.0651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.962</td>\n",
       "      <td>7460.84</td>\n",
       "      <td>1736.94</td>\n",
       "      <td>0.23280</td>\n",
       "      <td>52.4123</td>\n",
       "      <td>1.021020e+06</td>\n",
       "      <td>94.5234</td>\n",
       "      <td>2814.42</td>\n",
       "      <td>41</td>\n",
       "      <td>39.9147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45725</th>\n",
       "      <td>3.762</td>\n",
       "      <td>8037.12</td>\n",
       "      <td>2777.68</td>\n",
       "      <td>0.34560</td>\n",
       "      <td>64.3390</td>\n",
       "      <td>1.105797e+06</td>\n",
       "      <td>112.7460</td>\n",
       "      <td>3384.21</td>\n",
       "      <td>84</td>\n",
       "      <td>36.8036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45726</th>\n",
       "      <td>6.521</td>\n",
       "      <td>7978.76</td>\n",
       "      <td>2508.57</td>\n",
       "      <td>0.31440</td>\n",
       "      <td>75.8654</td>\n",
       "      <td>1.116725e+06</td>\n",
       "      <td>102.2770</td>\n",
       "      <td>3974.52</td>\n",
       "      <td>54</td>\n",
       "      <td>36.0470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45727</th>\n",
       "      <td>10.356</td>\n",
       "      <td>7726.65</td>\n",
       "      <td>2489.58</td>\n",
       "      <td>0.32220</td>\n",
       "      <td>70.9903</td>\n",
       "      <td>1.076560e+06</td>\n",
       "      <td>103.6780</td>\n",
       "      <td>3290.46</td>\n",
       "      <td>46</td>\n",
       "      <td>37.4718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45728</th>\n",
       "      <td>9.791</td>\n",
       "      <td>8878.93</td>\n",
       "      <td>3055.78</td>\n",
       "      <td>0.34416</td>\n",
       "      <td>94.0314</td>\n",
       "      <td>1.242266e+06</td>\n",
       "      <td>115.1950</td>\n",
       "      <td>3421.79</td>\n",
       "      <td>41</td>\n",
       "      <td>35.6045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45729</th>\n",
       "      <td>18.827</td>\n",
       "      <td>12732.40</td>\n",
       "      <td>4444.36</td>\n",
       "      <td>0.34905</td>\n",
       "      <td>157.6300</td>\n",
       "      <td>1.788897e+06</td>\n",
       "      <td>229.4590</td>\n",
       "      <td>4626.85</td>\n",
       "      <td>141</td>\n",
       "      <td>29.8118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45730 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         RMSD        F1       F2       F3        F4            F5        F6  \\\n",
       "0      17.284  13558.30  4305.35  0.31754  162.1730  1.872791e+06  215.3590   \n",
       "1       6.021   6191.96  1623.16  0.26213   53.3894  8.034467e+05   87.2024   \n",
       "2       9.275   7725.98  1726.28  0.22343   67.2887  1.075648e+06   81.7913   \n",
       "3      15.851   8424.58  2368.25  0.28111   67.8325  1.210472e+06  109.4390   \n",
       "4       7.962   7460.84  1736.94  0.23280   52.4123  1.021020e+06   94.5234   \n",
       "...       ...       ...      ...      ...       ...           ...       ...   \n",
       "45725   3.762   8037.12  2777.68  0.34560   64.3390  1.105797e+06  112.7460   \n",
       "45726   6.521   7978.76  2508.57  0.31440   75.8654  1.116725e+06  102.2770   \n",
       "45727  10.356   7726.65  2489.58  0.32220   70.9903  1.076560e+06  103.6780   \n",
       "45728   9.791   8878.93  3055.78  0.34416   94.0314  1.242266e+06  115.1950   \n",
       "45729  18.827  12732.40  4444.36  0.34905  157.6300  1.788897e+06  229.4590   \n",
       "\n",
       "            F7   F8       F9  \n",
       "0      4287.87  102  27.0302  \n",
       "1      3328.91   39  38.5468  \n",
       "2      2981.04   29  38.8119  \n",
       "3      3248.22   70  39.0651  \n",
       "4      2814.42   41  39.9147  \n",
       "...        ...  ...      ...  \n",
       "45725  3384.21   84  36.8036  \n",
       "45726  3974.52   54  36.0470  \n",
       "45727  3290.46   46  37.4718  \n",
       "45728  3421.79   41  35.6045  \n",
       "45729  4626.85  141  29.8118  \n",
       "\n",
       "[45730 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acb6df7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Note that we can but don't need to rescale\n",
    "y-values -- xGPR will rescale y-values automatically unless\n",
    "we tell it to do otherwise. The predictions are automatically\n",
    "converted back to the original scale. If you want to *stop* xGPR from\n",
    "rescaling y-values during training, you can pass `normalize_y=False`\n",
    "when constructing a dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "649cb19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(raw_data, test_size = 0.2, random_state=123)\n",
    "\n",
    "train_y, test_y = train_data[\"RMSD\"].values, test_data[\"RMSD\"].values\n",
    "train_x, test_x = train_data.iloc[:,1:].values, test_data.iloc[:,1:].values\n",
    "\n",
    "train_mean, train_std = train_x.mean(axis=0), train_x.std(axis=0)\n",
    "train_x = (train_x - train_mean[None,:]) / train_std[None,:]\n",
    "\n",
    "test_x = (test_x - train_mean[None,:]) / train_std[None,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c590e1cc",
   "metadata": {},
   "source": [
    "Next, we'll set the data up for use as a training set by xGPR. If \n",
    "the data is too large to fit in memory, we can save it in \"chunks\"\n",
    "to disk, each chunk as a .npy file with the corresponding y-values\n",
    "as another .npy file, then build an OfflineDataset.\n",
    "In this case, we'll build an OnlineDataset as well to illustrate.\n",
    "\n",
    "The chunk_size parameter indicates how much data the Dataset\n",
    "will feed to xGPR at any one given time during training. It's a \n",
    "little like a minibatch for deep learning. If you're using a\n",
    "large number of random features to ensure a highly accurate model,\n",
    "or if your data has a large number of features per datapoint,\n",
    "set chunk_size small to avoid excessive memory consumption. This\n",
    "does not affect the accuracy of the model or training in any way,\n",
    "merely memory and to some extent speed (larger chunk sizes are\n",
    "slightly faster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9498c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "online_train_data = build_online_dataset(train_x, train_y, chunk_size = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e576ee",
   "metadata": {},
   "source": [
    "For the OfflineDataset, we'll save the data to .npy files; each file\n",
    "can only contain up to chunk_size datapoints. skip_safety_checks\n",
    "defaults to False and when False checks the data to make sure there\n",
    "are no np.nan or np.inf, that all y-files have the same number of\n",
    "datapoints as corresponding x-files and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d3cd484",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 2000\n",
    "xfiles, yfiles = [], []\n",
    "\n",
    "for i in range(0, math.ceil(train_x.shape[0] / chunk_size)):\n",
    "    xfiles.append(f\"{i}_xblock.npy\")\n",
    "    yfiles.append(f\"{i}_yblock.npy\")\n",
    "    start = i * chunk_size\n",
    "    end = min((i + 1) * chunk_size, train_x.shape[0])\n",
    "    np.save(xfiles[-1], train_x[start:end,:])\n",
    "    np.save(yfiles[-1], train_y[start:end])\n",
    "\n",
    "#For OnlineDatasets, we always use build_online_dataset.\n",
    "#For OfflineDatasets, we use either build_offline_fixed_vector_dataset\n",
    "#(for tabular data) or build_offline_sequence_dataset (for sequences\n",
    "#and graphs).\n",
    "offline_train_data = build_offline_fixed_vector_dataset(xfiles, yfiles, chunk_size = 2000,\n",
    "                                                       skip_safety_checks = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bad2770",
   "metadata": {},
   "source": [
    "We'll tune hyperparameters using a Bayesian approach implemented under the class method ``crude_bayes``.\n",
    "For now, we'll use 2048 random features to tune hyperparameters\n",
    "and 8192 to fit the final model (fitting scales better to \n",
    "larger numbers of features and performance is more sensitive to the number\n",
    "used to fit than to the number used to tune, although increasing either\n",
    "boosts performance). For variance, 512 - 2048 random features is\n",
    "generally fine; this merely affects the accuracy with which uncertainty\n",
    "on predictions is quantified (more = better accuracy but more expensive).\n",
    "\n",
    "Note that \"crude_bayes\", like ``crude_grid`` and ``crude_lbfgs``,\n",
    "because it uses matrix decompositions, has very poor\n",
    "scaling to large numbers of random features. For < 5000 random features, it's reasonably\n",
    "fast on GPU (for CPU, even fewer is preferable). We'll show how to fine-tune the\n",
    "result from this procedure with a larger number of random features shortly.\n",
    "\n",
    "The \"subsample = 1\" argument is the default; it merely indicates we should use the whole\n",
    "dataset. \"subsample = 0.1\" would cause xGPR to randomly sample 10% of the training data\n",
    "when tuning, \"subsample = 0.5\" would sample 50% and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c8a2cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting_tuning\n",
      "Grid point 0 acquired.\n",
      "Grid point 1 acquired.\n",
      "Grid point 2 acquired.\n",
      "Grid point 3 acquired.\n",
      "Grid point 4 acquired.\n",
      "Grid point 5 acquired.\n",
      "Grid point 6 acquired.\n",
      "Grid point 7 acquired.\n",
      "Grid point 8 acquired.\n",
      "Grid point 9 acquired.\n",
      "New hparams: [-0.1576268]\n",
      "Additional acquisition 10.\n",
      "New hparams: [0.3133586]\n",
      "Additional acquisition 11.\n",
      "New hparams: [0.5691902]\n",
      "Additional acquisition 12.\n",
      "New hparams: [-0.7985376]\n",
      "Additional acquisition 13.\n",
      "New hparams: [-2.5544848]\n",
      "Additional acquisition 14.\n",
      "New hparams: [-4.5317151]\n",
      "Additional acquisition 15.\n",
      "New hparams: [0.5058531]\n",
      "Best score achieved: 38826.519\n",
      "Best hyperparams: [-0.4226011 -0.2897586  0.5058531]\n",
      "Tuning complete.\n",
      "Wallclock: 78.82700753211975\n"
     ]
    }
   ],
   "source": [
    "uci_model = xGPReg(training_rffs = 2048, fitting_rffs = 8192, variance_rffs = 1024,\n",
    "                  kernel_choice = \"RBF\", verbose = True, device = \"gpu\")\n",
    "\n",
    "start_time = time.time()\n",
    "uci_model.tune_hyperparams_crude_bayes(online_train_data, max_bayes_iter = 30, subsample = 1)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Wallclock: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205aeb4e",
   "metadata": {},
   "source": [
    "Just for fun, let's repeat this using the offline dataset...this requires\n",
    "loading data from disk in batches on each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4defbf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting_tuning\n",
      "Grid point 0 acquired.\n",
      "Grid point 1 acquired.\n",
      "Grid point 2 acquired.\n",
      "Grid point 3 acquired.\n",
      "Grid point 4 acquired.\n",
      "Grid point 5 acquired.\n",
      "Grid point 6 acquired.\n",
      "Grid point 7 acquired.\n",
      "Grid point 8 acquired.\n",
      "Grid point 9 acquired.\n",
      "New hparams: [-0.1576268]\n",
      "Additional acquisition 10.\n",
      "New hparams: [0.3133586]\n",
      "Additional acquisition 11.\n",
      "New hparams: [0.5691902]\n",
      "Additional acquisition 12.\n",
      "New hparams: [-0.7985376]\n",
      "Additional acquisition 13.\n",
      "New hparams: [-2.5544848]\n",
      "Additional acquisition 14.\n",
      "New hparams: [-4.5317151]\n",
      "Additional acquisition 15.\n",
      "New hparams: [0.5058531]\n",
      "Best score achieved: 38826.519\n",
      "Best hyperparams: [-0.4226011 -0.2897586  0.5058531]\n",
      "Tuning complete.\n",
      "Wallclock: 78.5753903388977\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "uci_model.tune_hyperparams_crude_bayes(offline_train_data, max_bayes_iter = 30)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Wallclock: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d2be6d",
   "metadata": {},
   "source": [
    "Finally, let's see what happens if we tune using L-BFGS with multiple restarts. This is usually slower (sometimes much slower) than ``crude_bayes`` but is a little more foolproof. You'll generally need to set\n",
    "n_restarts to some value like 3 or 5 -- L-BFGS is a local optimization strategy and\n",
    "can get trapped in poor local minima.\n",
    "\n",
    "Note that ``crude_lbfgs``, because it uses matrix decompositions, has very poor\n",
    "scaling to large numbers of random features. For < 5000 random features, it's reasonably\n",
    "fast on GPU (for CPU, even fewer is preferable). Like ``crude_bayes``, it's best\n",
    "as a way to find a starting point for futher optimization.\n",
    "We'll show how to fine-tune the result from this procedure with a larger number\n",
    "of random features shortly. Once again, just as for ``crude_bayes``, we can\n",
    "specify a ``subsample`` parameter if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6146b693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting_tuning\n",
      "Now beginning L-BFGS minimization.\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Restart 0 completed. Best score is 38815.89957910789.\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Restart 1 completed. Best score is 38815.89957910789.\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Evaluating gradient...\n",
      "Restart 2 completed. Best score is 38815.89957910789.\n",
      "Tuning complete.\n",
      "Wallclock: 184.239483833313\n"
     ]
    }
   ],
   "source": [
    "uci_model = xGPReg(training_rffs = 2048, fitting_rffs = 8192, variance_rffs = 1024,\n",
    "                  kernel_choice = \"RBF\", verbose = True, device = \"gpu\")\n",
    "\n",
    "start_time = time.time()\n",
    "uci_model.tune_hyperparams_crude_lbfgs(online_train_data, n_restarts = 3, subsample = 1)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Wallclock: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3832e2bb",
   "metadata": {},
   "source": [
    "We can retrieve the resulting hyperparameters and save them somewhere\n",
    "for future use if needed. This function always returns the\n",
    "log of the hyperparameters, and if you're passing\n",
    "hyperparameters to the fitting function, you should use\n",
    "the log of the hyperparameters as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2058512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.4167661 , -0.21041182,  0.40020105])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uci_model.get_hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a52c002",
   "metadata": {},
   "source": [
    "For fitting, we generally build a preconditioner first, unless\n",
    "fitting_rffs is small, in which case we can use mode = \"exact\"\n",
    "and fit with a single pass over the dataset, or if the dataset\n",
    "is small, in which case we can fit using mode = \"lbfgs\".\n",
    "For larger datasets and numbers of random features,\n",
    "preconditioned CG is our preferred option (sometimes\n",
    "stochastic gradient descent can also be competitive).\n",
    "\n",
    "method can be either 'srht' or 'srht_2'. 'srht' requires only\n",
    "one pass across the dataset, so it's\n",
    "pretty fast. 'srht_2' requires two passes across the dataset and\n",
    "involves matrix multiplication, so it's slower but the resulting\n",
    "preconditioner usually reduces the number of CG iterations required to\n",
    "fit by about 20-25% compared with 'srht', so it builds a better\n",
    "preconditioner. We recommend using 'srht_2' unless you're training on\n",
    "CPU, in which case 'srht_2' might be much slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18ae82bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 complete.\n",
      "Chunk 10 complete.\n",
      "Chunk 0 complete.\n",
      "Chunk 10 complete.\n",
      "Wallclock: 5.376834154129028\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "preconditioner, ratio = uci_model.build_preconditioner(offline_train_data, max_rank = 512,\n",
    "                                                      method = \"srht_2\")\n",
    "end_time = time.time()\n",
    "print(f\"Wallclock: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be473990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.153107742865895\n"
     ]
    }
   ],
   "source": [
    "print(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0cbc7d",
   "metadata": {},
   "source": [
    "Notice the \"ratio\" (aka Min eigval / lambda**2). The smaller\n",
    "this value, the fewer iterations CG will need to fit. See the\n",
    "\"Fitting\" section of the docs for some guidance on what is a \n",
    "\"good-enough\" ratio (i.e. a ratio that will result in a fast fit).\n",
    "\n",
    "The smaller tol, the tighter\n",
    "the fit and the more accurate the model, but there are sharply\n",
    "diminishing returns on this -- past a certain point, decreasing\n",
    "tol just increases the number of iterations required to fit\n",
    "while providing a very slight benefit. 1e-6 is usually more\n",
    "than enough for noisy data; for noise-free data, 1e-7 is a good\n",
    "setting; 1e-8 is usually very expensive overkill. For\n",
    "datasets where the data is virtually noise-free and you need\n",
    "a really tight fit to minimize error as much as possible\n",
    "1e-8 may sometimes be useful; just keep in mind it will\n",
    "greatly increase the number of iterations required to fit\n",
    "and hence fitting time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e12d8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting fitting\n",
      "Iteration 0\n",
      "Iteration 5\n",
      "Iteration 10\n",
      "Iteration 15\n",
      "Iteration 20\n",
      "Iteration 25\n",
      "Iteration 30\n",
      "Now performing variance calculations...\n",
      "Fitting complete.\n",
      "Wallclock: 5.012384414672852\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "uci_model.fit(offline_train_data, preconditioner = preconditioner,\n",
    "             mode = \"cg\", tol = 1e-6)\n",
    "end_time = time.time()\n",
    "print(f\"Wallclock: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80bcc80",
   "metadata": {},
   "source": [
    "We can get the uncertainty on predictions by setting get_var = True.\n",
    "In this case, we don't need it, so we'll skip it. chunk_size ensures\n",
    "we only process up to chunk_size datapoints at one time to limit\n",
    "memory consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66136407",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions, test_var = uci_model.predict(test_x, get_var = True, chunk_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6e1aef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 2.9686757776170625\n"
     ]
    }
   ],
   "source": [
    "mae = np.mean( np.abs(test_predictions - test_y))\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6655429c",
   "metadata": {},
   "source": [
    "Suppose we are unhappy with this result. We could of course consider\n",
    "a different kernel or modeling approach; alternatively, we can\n",
    "increase the number of random features for either tuning or\n",
    "fitting, which will almost invariably improve performance.\n",
    "\n",
    "Tuning hyperparameters with ``crude`` methods\n",
    "is a quick and dirty approach that does not scale well to large numbers of random\n",
    "features. If we want to increase the number of features used for tuning to\n",
    "more than 4096 (or to more than 2048 on CPU) we should consider using\n",
    "approximate marginal likelihood instead. This is slow but has much better\n",
    "scaling (the increase in cost with a larger number of random features is\n",
    "much smaller).\n",
    "\n",
    "Generally increasing the number of random features used for fitting gives a\n",
    "bigger performance boost than increasing the number for tuning. For fitting,\n",
    "it can be beneficial to use as many as 32,768 random features, while\n",
    "for tuning, we seldom see large performance gains for more than 10,000.\n",
    "Either way, however, increasing the number of random features\n",
    "yields diminishing returns. Going from 1024 to 2048 gives\n",
    "a more substantial improvement than going from 2048 to\n",
    "4096, and so on. If you ever find yourself needing to\n",
    "go to very high numbers, the model & kernel may not be\n",
    "a good fit for that particular problem.\n",
    "\n",
    "First, let's increase the number used to fit and see what happens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65ceb406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 complete.\n",
      "Chunk 10 complete.\n",
      "Chunk 0 complete.\n",
      "Chunk 10 complete.\n",
      "Wallclock: 20.81070065498352\n"
     ]
    }
   ],
   "source": [
    "uci_model.fitting_rffs = 32768\n",
    "\n",
    "start_time = time.time()\n",
    "preconditioner, ratio = uci_model.build_preconditioner(offline_train_data, max_rank = 512,\n",
    "                                                      method = \"srht_2\")\n",
    "end_time = time.time()\n",
    "print(f\"Wallclock: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0f76c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting fitting\n",
      "Iteration 0\n",
      "Iteration 5\n",
      "Iteration 10\n",
      "Iteration 15\n",
      "Iteration 20\n",
      "Iteration 25\n",
      "Iteration 30\n",
      "Now performing variance calculations...\n",
      "Fitting complete.\n",
      "Wallclock: 16.371665000915527\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "uci_model.fit(offline_train_data, preconditioner = preconditioner,\n",
    "             mode = \"cg\", tol = 1e-6)\n",
    "end_time = time.time()\n",
    "print(f\"Wallclock: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "548d75f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 2.875096362754352\n"
     ]
    }
   ],
   "source": [
    "test_predictions = uci_model.predict(test_x, get_var = False, chunk_size = 1000)\n",
    "mae = np.mean( np.abs(test_predictions - test_y))\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff215bc",
   "metadata": {},
   "source": [
    "As discussed above, we could also retune hyperparameters using a larger number of random features, preferably\n",
    "using approximate marginal likelihood. There are two strategies for doing this implemented in xGPR: a Bayesian strategy (``uci_model.tune_hyperparams_fine_bayes``) or a direct strategy (``uci_model.tune_hyperparams_fine_direct``, which uses either the \"Powell\" algorithm or \"Nelder-Mead\"). There are more \"knobs\" that have to be set correctly\n",
    "to ensure the approximation is calculated correctly; we'll discuss a few of these briefly here, but see the Tuning section of the docs for more. Also see the small molecule example for another illustration on a problem involving a graph convolution kernel.\n",
    "\n",
    "``fine_direct`` with \"Powell\", is faster, but for it to work well, the starting point has to be \"within sight\" of the global optimum, whereas ``fine_bayes`` can find the global optimum as long as it's somewhere in the neighborhood. ``fine_direct`` with ``optim_method=\"Nelder-Mead\"`` is a little bit of a wild card that usually takes many more iterations than Powell but often works slightly better. ``fine_bayes`` is also only applicable for kernels with 3 or fewer hyperparameters (e.g. RBF, FHTConv1d, Matern, GraphConv1d).\n",
    "\n",
    "Tuning with up to 35 iterations will essentially work out to\n",
    "fitting the model up to 35x, so you can gauge how long this will take based on how long it took to build a preconditioner and fit once. As discussed, this approach is slower -- but also more scalable -- than ``crude_bayes``. Increasing max_iter will increase the chances of finding the best possible hyperparameters, but may of course take longer.\n",
    "\n",
    "The same settings that work well for fitting generally work well for ensuring the marginal likelihood approximation used by this function is accurate. Since we used ``tol`` of 1e-6 for fitting, we'll use ``nmll_tol = 1e-6`` here as well. It is important not to use an ``nmll_rank`` that's too small -- 1024 is generally a decent default, but if you need to use a larger value than that to get a fast fit, you should probably use the same value here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7208786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting_tuning\n",
      "Now beginning NM minimization.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Now building preconditioner...\n",
      "Now fitting...\n",
      "NMLL evaluation completed.\n",
      "Tuning complete.\n",
      "Wallclock: 655.230809211731\n"
     ]
    }
   ],
   "source": [
    "uci_model.training_rffs = 8192\n",
    "\n",
    "start_time = time.time()\n",
    "start_hparams = np.array([-0.41722132,  0.20104138,  0.39762501])\n",
    "\n",
    "uci_model.tune_hyperparams_fine_direct(offline_train_data, starting_hyperparams = start_hparams,\n",
    "                                  optim_method = \"Powell\",\n",
    "                                  random_seed = 123, max_iter = 40,\n",
    "                                  nmll_tol = 1e-6, nmll_rank = 1024)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Wallclock: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afb6c7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.49860283 -0.06108382  0.6371851 ]\n"
     ]
    }
   ],
   "source": [
    "print(uci_model.get_hyperparams())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf5f026",
   "metadata": {},
   "source": [
    "Now let's refit using our new hyperparameters, using 8192 fitting rffs so we \n",
    "can compare to what we used at first. We'll see that we get a slight improvement\n",
    "over our initial tuning run, but nothing to write home about. (This isn't always\n",
    "true -- see the small molecule example for a case where fine-tuning provides\n",
    "a more substantial boost.) Of course,\n",
    "by fitting using this new hyperparameter set with 32768 RFFs instead of 8192\n",
    "we could get some additional improvement. Random features offer asymptotic improvement --\n",
    "we are asymptotically approaching what we would get with a (much more expensive) exact GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90fbdc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 complete.\n",
      "Chunk 10 complete.\n",
      "Chunk 0 complete.\n",
      "Chunk 10 complete.\n",
      "starting fitting\n",
      "Iteration 0\n",
      "Iteration 5\n",
      "Iteration 10\n",
      "Iteration 15\n",
      "Iteration 20\n",
      "Iteration 25\n",
      "Iteration 30\n",
      "Iteration 35\n",
      "Iteration 40\n",
      "Iteration 45\n",
      "Now performing variance calculations...\n",
      "Fitting complete.\n",
      "2.884159125457879\n"
     ]
    }
   ],
   "source": [
    "uci_model.fitting_rffs = 8192\n",
    "preconditioner, ratio = uci_model.build_preconditioner(online_train_data, max_rank = 512,\n",
    "                                                      method = \"srht_2\")\n",
    "uci_model.fit(online_train_data, preconditioner = preconditioner,\n",
    "             mode = \"cg\", tol = 1e-6)\n",
    "test_predictions = uci_model.predict(test_x, get_var = False, chunk_size = 1000)\n",
    "mae = np.mean( np.abs(test_predictions - test_y))\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3d3fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can switch the model over to CPU if we want to do inference on CPU (training is best\n",
    "#done on GPU if possible.)\n",
    "uci_model.device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11e757eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we'll delete the .npy files we created earlier.\n",
    "offline_train_data.delete_dataset_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa66e48",
   "metadata": {},
   "source": [
    "That's it for this simple warm-up. Now let's look at some more\n",
    "interesting examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141e83df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
