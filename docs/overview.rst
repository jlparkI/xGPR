Overview of xGPR
===============================================

To fit a regression model in xGPR, we need to do the following
four things.

  + Construct a Dataset object which contains the training data as numpy arrays
    (if it's small enough to load into memory) or as a list of filepaths for
    .npy files on disk. See :doc:`Getting started</initialization_tutorial>`.
  
  + Choose a kernel. Unlike any other GP library, xGPR includes convolution
    kernels for sequences and graphs that are both efficient and powerful.
    For available kernel options, see the Kernels subsection on the
    main page.

  + Tune kernel hyperparameters on the training dataset. See
    :doc:`Tuning hyperparameters</tuning_tutorial>`.

  + Fit the model to the training data. See
    :doc:`Fitting a model</fitting_tutorial>`.


If you want to run approximate kernel k-means or kernel PCA on your data, this is
just a one-step process:

  + Construct 


The Tutorials and Examples explain each step in greater detail. If you're
like us, you might find it helpful to look at the examples first,
then head back to the tutorials for a more in-depth overview. Before reading
anything else, though, it's useful to know a little more about random features.

What are "random features"?
---------------------------

An exact Gaussian process constructs a covariance matrix across the entire
training set by measuring the similarity of each datapoint to every other
training datapoint using the specified kernel function. This is very
accurate but has quadratic scaling and is completely impractical for large
datasets (consider constructing a one million by one million matrix for
a million-datapoint dataset!)

xGPR approximates the exact GP by representing each datapoint using a
set of "random features", a representation constructed such that the
dot product between any two representations is approximately equivalent
to the exact kernel measurement. This converts a Gaussian process
into (essentially) a ridge regression problem. We use a number of
"tricks" to speed up random feature generation and fitting, but
this is the general idea. Note that
this representation -- just like the "embedding" from a neural network --
can be used for clustering and for visualization (running PCA on
the representation generated by the kernel approximates running
kernel PCA on the original data, and running k-means on the
representation approximates running kernel k-means on the original
data).

The more random features you use, the more accurate the approximation
and the closer to an exact GP. The error decreases exponentially
with the number of random features, so that if going from 1024 to
2048 random features improves model accuracy by a lot, going from
2048 to 4096 will improve it but not by as much, and then going from
4096 to 8192 will improve it but still less, and so on. In other words,
increasing the number of random features provides diminishing returns.
The graph below illustrates how mean absolute error on held-out test
sets for some (fairly random) UCI machine learning datasets decreases
as the number of random features used for tuning hyperparameters
or for fitting the model increases.

.. image:: images/performance_vs_rffs.png

Notice that increasing the number of random features is nearly always
beneficial, but with diminishing returns. This means that if our
xGPR model is performing pretty well but we'd like it to perform
a little better, we have a straightforward way to achieve this --
just use more random features. If perforance is dramatically below
what we need it to be, by contrast, and we're already using a reasonable
number of random features, we know that (because increasing the number
of RFFs provides diminishing returns) we need to switch to a different
kernel, feature set or modeling technique.

How many random features do I need?
------------------------------------

When working with a new dataset, you need to determine quickly what
approach is most likely to work, which means you may need to screen
different models (e.g. xGPR and xGBoost) and for xGPR, you may want
to screen different kernels and/or different feature representations
of the data.

Initially, then, you want to get a quick and dirty readout
on how well a given kernel or feature representation is likely to
perform. For these initial experiments, we suggest using a small
number of random features to tune hyperparameters (e.g. 512 - 2048).
While larger numbers of random features will provde much better performance,
this quick initial screen can help you decide which feature set
or kernel you want, and it will give you a starting set of hyperparameters
that can be "fine-tuned" with a larger number of random features later.

Once you've decided on a kernel, feature set etc, you'll want to
tune the hyperparameters using a larger number of random features
to get best performance and fit the model using a larger number
of random features as well. There are three approaches for tuning --
we'll explore how these work in the Tutorials. Note from the figure
above that performance gains with increasing random features
saturate for hyperparameter tuning more quickly than for
fitting.

For fitting, we've
generally found that 8192 - 16384 (or a similar number, we like to use
powers of two but this is not required) random features provides good
performance. Again, it's always possible to
improve performance a little by using more -- it really depends
on whether the performance gain is worth the additional cost
for a specific application.

The variance (i.e. the uncertainty) on new predictions is a useful
quantity, but it generally only needs to be calculated very
approximately. We recommend using 512 - 2048 random features for
variance; this should be sufficient for an estimate in most cases.
We often use 1024.
