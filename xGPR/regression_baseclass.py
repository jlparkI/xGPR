"""Describes the GPRegressionBaseclass from which other model classes inherit.

The GPRegressionBaseclass describes class attributes and methods shared by
model classes like xGPRegression.
"""
import os
import sys
import copy
import warnings
import abc
from abc import ABC

try:
    import cupy as cp
except:
    pass
import numpy as np
from scipy.optimize import minimize

from .data_handling.dataset_builder import build_offline_fixed_vector_dataset
from .kernels import KERNEL_NAME_TO_CLASS
from .constants import constants
from .optimizers.stochastic_optimizer import amsgrad_optimizer
from .optimizers.pure_bayes_optimizer import pure_bayes_tuning
from .optimizers.bayes_grid_optimizer import bayes_grid_tuning
from .optimizers.lb_optimizer import shared_hparam_search
from .optimizers.crude_grid_optimizer import crude_grid_tuning

class GPRegressionBaseclass(ABC):
    """The base class for xGPR regression classes. Provides shared
    methods and attributes.

    Attributes:
        kernel_choice (str): The kernel selected by the user.
        kernel: The kernel object for the posterior predictive mean. The class of
            this object will depend on the kernel specified by the user -- e.g.
            the 'rbf' kernel corresponds to class Rbf.
        weights: A 1d array, either a cp.ndarray or np.ndarray depending on the
            device specified by the user (cpu or gpu). The random features
            generated by self.kernel are multiplied against the weights to
            generate the posterior predictive mean. The weights are calculated
            during fitting.
        var: A 2d square array, either a cp.ndarray or np.ndarray depending on
            the device specified by the user (cpu or gpu). The random features
            are used in conjunction with var to generate the posterior predictive
            variance. The var is calculated during fitting.
        device (str): One of "gpu", "cpu". The user can update this as desired.
            All predict / tune / fit operations are carried out using the
            current device.
        training_rffs (int): The number of random Fourier features used for
            tuning the hyperparameters.
        fitting_rffs (int): The number of random Fourier features used for
            fitting the model and generating the posterior predictive mean.
        variance_rffs (int): The number of random Fourier features used for
            calculating posterior predictive variance.
        kernel_specific_params (dict): Contains kernel-specific parameters --
            e.g. 'matern_nu' for the nu for the Matern kernel, or 'conv_width'
            for the conv1d kernel.
        verbose (bool): If True, regular updates are printed during
            hyperparameter tuning and fitting.
        double_precision_fht (bool): If False, use single precision floats to generate
            random features. This can increase speed but may result in a slight (usually
            negligible) loss of accuracy.
        trainy_mean (float): The mean of the training ydata. Determined during fitting.
            Used for making predictions.
        trainy_std (float): The standard deviation of the training ydata. Determined
            during fitting. Used for making predictions.
    """

    def __init__(self, training_rffs,
                    fitting_rffs,
                    variance_rffs = 16,
                    kernel_choice="RBF",
                    device = "cpu",
                    kernel_specific_params = constants.DEFAULT_KERNEL_SPEC_PARMS,
                    verbose = True,
                    double_precision_fht = False):
        """Constructor.

        Args:
            training_rffs (int): The number of random Fourier features
                to use for hyperparameter tuning.
            fitting_rffs (int): The number of random Fourier features
                to use for posterior predictive mean (i.e. the predicted
                value for new datapoints).
            variance_rffs (int): The number of random Fourier features
                to use for posterior predictive variance (i.e. calculating
                uncertainty on predictions). Defaults to 64.
            kernel_choice (str): The kernel that the model will use.
                Defaults to 'RBF'. Must be in kernels.kernel_list.
                KERNEL_NAME_TO_CLASS.
            device (str): Determines whether calculations are performed on
                'cpu' or 'gpu'. The initial entry can be changed later
                (i.e. model can be transferred to a different device).
                Defaults to 'cpu'.
            kernel_specific_params (dict): Contains kernel-specific parameters --
                e.g. 'matern_nu' for the nu for the Matern kernel, or 'conv_width'
                for the conv1d kernel.
            verbose (bool): If True, regular updates are printed
                during fitting and tuning. Defaults to True.
            double_precision_fht (bool): If False, use single precision floats to generate
                random features. This can increase speed but may result in a slight (usually
                negligible) loss of accuracy.
        """
        self.kernel_choice = kernel_choice
        self.kernel = None
        self.weights = None
        self.var = None
        self.device = device

        self.training_rffs = training_rffs
        self.fitting_rffs = fitting_rffs
        #Variance_rffs must be <= fitting_rffs. Always set second
        self.variance_rffs = variance_rffs

        self.kernel_spec_parms = kernel_specific_params
        self.double_precision_fht = double_precision_fht

        self.verbose = verbose
        self.trainy_mean = 0.0
        self.trainy_std = 1.0


    #First, abstract methods required for subclasses.

    @abc.abstractmethod
    def exact_nmll_gradient(self, hyperparams, dataset):
        """Subclasses must implement a method that calculates
        the gradient of the negative marginal log likelihood."""


    @abc.abstractmethod
    def minibatch_nmll_gradient(self, params, xdata, ydata):
        """Subclasses must implement a method that calculates
        the gradient of the negative marginal log likelihood for
        a minibatch."""

    @abc.abstractmethod
    def exact_nmll(self, hyperparams, dataset):
        """Subclasses must implement a method that calculates
        the negative marginal log likelihood."""

    @abc.abstractmethod
    def approximate_nmll(self, hyperparams, dataset, max_rank = None,
            nsamples = 25, random_seed = 123, niter = 75,
            tol = 1e-5, pretransform_dir = None):
        """Subclasses must implement a method that estimates the
        negative marginal log likelihood."""


    @abc.abstractmethod
    def _calc_weights_exact(self, dataset):
        """Child must implement a method that can
        calculate the weights using matrix decomposition when
        self.fitting_rffs is sufficiently small."""

    @abc.abstractmethod
    def _calc_weights_cg(self, dataset, cg_tol = 1e-5,
                        max_iter = 500, preconditioner = None):
        """Child must implement a method that can
        calculate the weights using conjugate gradients
        using the internal implementation. This is deprecated
        and will be retired in a future release."""

    @abc.abstractmethod
    def _calc_weights_cg_lib_ext(self, dataset, cg_tol = 1e-5,
                        max_iter = 500, preconditioner = None):
        """Child must implement a method that can
        calculate the weights using conjugate gradients with
        the scipy / cupy implementation."""

    @abc.abstractmethod
    def _calc_weights_lbfgs(self, fitting_dataset, tol, max_iter = 500):
        """Child must implement a method that can calculate the weights
        using L-BFGS."""



    @abc.abstractmethod
    def _calc_weights_sgd(self, fitting_dataset, tol, max_iter = 100,
            preconditioner = None, manual_lr = None):
        """Child must implement a method that can calculate the weights
        using SGD."""

    @abc.abstractmethod
    def _calc_weights_ams(self, fitting_dataset, tol, max_iter = 50):
        """Child must implement a method that can calculate the weights
        using amsgrad."""


    @abc.abstractmethod
    def _calc_variance(self, dataset):
        """Subclasses must implement a method that can
        calculate the variance matrix (self.var)."""


    def suggest_bounds(self, box_width = 1.0, consider_all_shared = False):
        """Returns suggested boundaries for hyperparameter tuning.
        This should be called after initial "crude"
        tuning has been conducted before more expensive "fine-
        tuning" is conducted.

        Args:
            box_width (int): The width of the box to draw for
                kernel-specific hyperparameters. For shared
                hyperparameters (noise, amplitude) the box
                width is determined automatically.
            consider_all_shared (bool): If True, for lambda and beta
                (the two shared hyperparameters), the box boundaries
                are set at the default boundaries for the kernel.
                This can sometimes be beneficial since many "crude"
                methods are better at finding good values for the
                kernel-specific parameter (lengthscale) than for lambda /
                beta.

        Returns:
            bounds (array): A numpy array of shape (n, 2),
                where n is the number of hyperparameters. If
                tuning has not yet been conducted, a warning
                is generated and None is returned instead. Passing
                None as bounds to a hyperparameter tuning
                function will just result in using the
                specified kernel's defaults."""
        if self.kernel is None:
            warnings.warn("Tuning has not yet been performed. "
                    "The kernel default boundaries are suggested. "
                    "Returning None.")
            return None
        start_pt = self.kernel.get_hyperparams(logspace = True)
        #Round to nearest decimal place to avoid any minor
        #fluctuations in boundaries stemming from slight
        #adjustments to crude tuning strategy.
        start_pt = np.round(start_pt, 1)
        default_bounds = self.kernel.get_bounds(logspace = True)

        suggested_bounds = []
        for i in range(start_pt.shape[0]):
            suggested_bounds.append([start_pt[i] - box_width,
                start_pt[i] + box_width])

        suggested_bounds = np.asarray(suggested_bounds)
        suggested_bounds[:,0] = np.max([suggested_bounds[:,0],
                        default_bounds[:,0]], axis=0)
        suggested_bounds[:,1] = np.min([suggested_bounds[:,1],
                        default_bounds[:,1]], axis=0)
        if consider_all_shared:
            suggested_bounds[0:2,:] = default_bounds[0:2,:]
        return suggested_bounds



    def _initialize_kernel(self, kernel_choice, input_dims, num_rffs,
                        random_seed, bounds = None):
        """Selects and initializes an appropriate kernel object based on the
        kernel_choice string supplied by caller. The kernel is then moved to
        the appropriate device based on the 'device' supplied by caller
        and is returned.

        Args:
            kernel_choice (str): The kernel selection. Must be one of
                constants.ACCEPTABLE_KERNELS.
            input_dims (list): The dimensions of the data. A list of
                [ndatapoints, x.shape[1]] for non-convolution data
                or [ndatapoints, x.shape[1], x.shape[2]] for conv1d
                data.
            num_rffs (int): The number of random features the kernel
                object should generate.
            random_seed (int): The random seed to the random number
                generator the kernel uses to initialize.
            bounds (np.ndarray): The bounds on hyperparameter
                tuning. Must have an appropriate shape for the
                selected kernel. If None, the kernel will use
                its defaults. Defaults to None.

        Returns:
            kernel: An object of the appropriate kernel class.

        Raises:
            ValueError: Raises a value error if an unrecognized kernel
                is supplied.
        """
        if kernel_choice not in KERNEL_NAME_TO_CLASS:
            raise ValueError("An unrecognized kernel choice was supplied.")
        kernel = KERNEL_NAME_TO_CLASS[kernel_choice](input_dims,
                            num_rffs, random_seed, self.device,
                            self.double_precision_fht,
                            kernel_spec_parms = self.kernel_spec_parms)
        if bounds is not None:
            kernel.set_bounds(bounds)
        return kernel


    def _pretransform_dataset(self, dataset, pretransform_location):
        """Pretransforms a dataset, generating the random features without
        applying the activation function then saving these pre-generated
        features (to which only the final processing step needs to be applied)
        to disk. If the number of features is small but the number of datapoints
        is large, or if an SSD hard drive is available, this can bring about
        a substantial speedup.

        Args:
            dataset: An object of class CPUOnlineDataset, CPUOfflineDataset,
                GPUOnlineDataset or GPUOfflineDataset. Contains the prechecked
                data as either a list of on-disk arrays or a set of numpy arrays.
            pretransform_location (str): A valid filepath to a directory where
                the pretransformed arrays can be saved.
        """
        if self.verbose:
            print("Now pretransforming data.")
        if self.kernel is None:
            raise ValueError("Tried to pretransform data without an initialized kernel.")
        current_dir = os.getcwd()
        os.chdir(pretransform_location)
        xfiles, yfiles = [], []
        max_chunk_size = 0

        for i, (xbatch, ybatch) in enumerate(dataset.get_chunked_data()):
            xfile, yfile = f"PRETRANSFORMED_{i}_X.npy", f"PRETRANSFORMED_{i}_Y.npy"
            xbatch = self.kernel.transform_x(xbatch)
            if self.device == "gpu":
                xbatch = cp.asnumpy(xbatch).astype(np.float32)
                ybatch = cp.asnumpy(ybatch)
            else:
                xbatch = xbatch.astype(np.float32)
            ybatch = ybatch * dataset.get_ystd() + dataset.get_ymean()
            np.save(xfile, xbatch)
            np.save(yfile, ybatch)
            xfiles.append(xfile)
            yfiles.append(yfile)
            max_chunk_size = max(max_chunk_size, xbatch.shape[0])

        tuning_dataset = build_offline_fixed_vector_dataset(xfiles,
                            yfiles, chunk_size = max_chunk_size,
                            skip_safety_checks = True)
        tuning_dataset.pretransformed = True
        tuning_dataset.parent_xdim = dataset.get_xdim()

        os.chdir(current_dir)
        return tuning_dataset

    #############################
    #The next block of functions are used for tuning hyperparameters. We provide
    #a variety of strategies (the user can additionally combine them
    #to generate more), with some strong recommendations on which to use when
    #(see docs). The bulk of the code in this class is dedicated to these
    #various strategies.
    #############################



    def tune_hyperparams_fine_bayes(self, dataset, bounds = None, random_seed = 123,
                    max_bayes_iter = 30, tol = 1e-1, nmll_rank = 1024,
                    nmll_probes = 25, nmll_iter = 500,
                    nmll_tol = 1e-6, pretransform_dir = None,
                    preconditioner_mode = "srht_2"):
        """Tunes the hyperparameters using Bayesian optimization, WITHOUT
        using gradient information. This algorithm is not very efficient for searching the entire
        hyperparameter space, BUT for 3 and 4 hyperparameter kernels, it can be very effective
        for searching some bounded region. Consequently, it may sometimes be useful to
        use a "crude" method (e.g. minimal_bayes) to find a starting point,
        then run this method to fine-tune.

        This method uses approximate NMLL. Note that the approximation will only be
        good so long as the 'nmll' settings selected are reasonable.

        Args:
            dataset: Object of class OnlineDataset or OfflineDataset.
            bounds (np.ndarray): The bounds for optimization. Must be supplied,
                in contrast to most other tuning routines, since this routine
                is more seldom used for searching the whole hyperparameter space.
                Must be a 2d numpy array of shape (num_hyperparams, 2).
            random_seed (int): A random seed for the random
                number generator. Defaults to 123.
            max_bayes_iter (int): The maximum number of iterations of Bayesian
                optimization.
            tol (float): Criteria for convergence.
            nmll_rank (int): The preconditioner rank for approximate NMLL estimation.
                A larger value may reduce the number of iterations for nmll approximation
                to converge and improve estimation accuracy, but will also increase
                cost for preconditioner construction.
            nmll_probes (int): The number of probes for approximate NMLL estimation.
                A larger value may improve accuracy of estimation but with increased
                computational cost.
            nmll_iter (int): The maximum number of iterations for approximate NMLL.
                A larger value may improve accuracy of estimation but with
                increased computational cost.
            nmll_tol (float): The convergence tolerance for approximate NMLL.
                A smaller value may improve accuracy of estimation but with
                increased computational cost.
            pretransform_dir (str): Either None or a valid filepath where pretransformed
                data can be saved. If not None, the dataset is "pretransformed" before
                each round of evaluations when using approximate NMLL. This can take up a
                lot of disk space if the number of random features is large, but can
                greatly increase speed of fitting for convolution kernels with large #
                random features.
            preconditioner_mode (str): One of "srht", "srht_2". Determines the mode of
                preconditioner construction. "srht" is cheaper, requiring one pass over
                the dataset, but lower quality. "srht_2" requires two passes over the
                dataset but is better. Prefer "srht_2" unless you are running on CPU,
                in which case "srht" may be preferable.

        Returns:
            hyperparams (np.ndarray): The best hyperparams found during optimization.
            n_feval (int): The number of function evaluations during optimization.
            best_score (float): The best negative marginal log-likelihood achieved.

        Raises:
            ValueError: The input dataset is checked for validity before tuning is
                initiated. If problems are found, a ValueError will provide an
                explanation of the error. This method will also raise a ValueError
                if you try to use it on a kernel with > 4 hyperparameters (since
                Bayesian tuning loses efficiency in high dimensions rapidly).
        """
        if nmll_rank >= self.training_rffs:
            raise ValueError("NMLL rank must be < the number of training rffs.")
        bounds = self._run_pretuning_prep(dataset, random_seed, bounds, "approximate")
        nmll_params = (nmll_rank, nmll_probes, random_seed,
                    nmll_iter, nmll_tol, pretransform_dir,
                    preconditioner_mode)

        hyperparams, best_score, n_feval = pure_bayes_tuning(self.approximate_nmll,
                        dataset, bounds, random_seed,
                        max_iter = max_bayes_iter,
                        verbose = self.verbose, tol = tol,
                        nmll_params = nmll_params)
        self._post_tuning_cleanup(dataset, hyperparams)
        return hyperparams, n_feval, best_score




    def tune_hyperparams_crude_grid(self, dataset, bounds = None, random_seed = 123,
                    n_gridpoints = 30, n_pts_per_dim = 10, subsample = 1,
                    eigval_quotient = 1e6, min_eigval = 1e-5):
        """Tunes the hyperparameters using gridsearch, but with
        a 'trick' that simplifies the problem greatly for 2-3 hyperparameter
        kernels. Hyperparameters are scored using an exact NMLL calculation.
        The NMLL calculation uses a matrix decomposition with cubic
        scaling in the number of random features, so it is extremely slow
        for anything more than 3-4,000 random features, but has
        low risk of overfitting and is easy to use. It is therefore
        intended as a "quick-and-dirty" method. We recommend using
        this with a small number of random features (e.g. 500 - 3000)
        as a starting point for fine-tuning.

        Args:
            dataset: Object of class OnlineDataset or OfflineDataset.
            bounds (np.ndarray): The bounds for optimization. If None, default
                boundaries for the kernel will be used. Otherwise, must be an
                array of shape (# hyperparams, 2).
            random_seed (int): A random seed for the random
                number generator. Defaults to 123.
            n_gridpoints (int): The number of gridpoints per non-shared hparam.
            n_pts_per_dim (int): The number of grid points per shared hparam.
            subsample (float): A value in the range [0.01,1] that indicates what
                fraction of the training set to use each time the score is
                calculated (the same subset is used every time). In general, 1
                will give better results, but using a subsampled subset can be
                a fast way to find the (approximate) location of a good
                hyperparameter set.
            eigval_quotient (float): A value by which the largest singular value
                of Z^T Z is divided to determine the smallest acceptable eigenvalue
                of Z^T Z (singular vectors with eigenvalues smaller than this
                are discarded). Setting this to larger values will make this
                slightly more accurate, at the risk of numerical stability issues.
                In general, do not change this without good reason.
            min_eigval (float): If the largest singular value of Z^T Z divided by
                eigval_quotient is < min_eigval, min_eigval is used as the cutoff
                threshold instead. Setting this to smaller values will make this
                slightly more accurate, at the risk of numerical stability
                issues. In general, do not change this without good reason.

        Returns:
            hyperparams (np.ndarray): The best hyperparams found during optimization.
            n_feval (int): The number of function evaluations during optimization.
            best_score (float): The best negative marginal log-likelihood achieved.
            scores (tuple): A tuple where the first element is the sigma values
                evaluated and the second is the resulting scores. Can be useful
                for diagnostic purposes.

        Raises:
            ValueError: The input dataset is checked for validity before tuning is
                initiated. If problems are found, a raise exception will provide an
                explanation of the error. This method will also raise an exception
                if you try to use it on a kernel with > 4 hyperparameters (since
                this strategy no longer provides any benefit under those conditions)
                or < 3.
            n_init_pts (int): The number of initial grid points to evaluate before
                Bayesian optimization. 10 (the default) is usually fine. If you are
                searcing a smaller space, however, you can save time by using
                a smaller # (e.g. 5).
        """
        bounds = self._run_pretuning_prep(dataset, random_seed, bounds, "exact")
        num_hparams = self.kernel.get_hyperparams().shape[0]
        if num_hparams == 2:
            best_score, hyperparams = shared_hparam_search(np.array([]), self.kernel,
                    dataset, bounds, n_pts_per_dim, 3, subsample = subsample)
            n_feval, scores = 1, ()
        elif num_hparams == 3:
            hyperparams, scores, best_score = crude_grid_tuning(self.kernel,
                                dataset, bounds, self.verbose,
                                n_gridpoints, subsample = subsample,
                                eigval_quotient = eigval_quotient,
                                min_eigval = min_eigval)
            n_feval = n_gridpoints

        else:
            raise ValueError("The crude grid procedure is only appropriate for "
                    "kernels with 2-3 hyperparameters.")

        self._post_tuning_cleanup(dataset, hyperparams)
        return hyperparams, n_feval, best_score, scores




    def tune_hyperparams_crude_bayes(self, dataset, bounds = None, random_seed = 123,
                    max_bayes_iter = 30, bayes_tol = 1e-1, n_pts_per_dim = 10,
                    n_cycles = 3, n_init_pts = 10, subsample = 1,
                    eigval_quotient = 1e6, min_eigval = 1e-5):
        """Tunes the hyperparameters using Bayesian optimization, but with
        a 'trick' that simplifies the problem greatly for 2-4 hyperparameter
        kernels. Hyperparameters are scored using an exact NMLL calculation.
        The NMLL calculation uses a matrix decomposition with cubic
        scaling in the number of random features, so it is extremely slow
        for anything more than 3-4,000 random features, but has
        low risk of overfitting and is easy to use. It is therefore
        intended as a "quick-and-dirty" method. We recommend using
        this with a small number of random features (e.g. 500 - 3000)
        and if performance is insufficient, fine-tune hyperparameters
        using a method with better scalability.

        Args:
            dataset: Object of class OnlineDataset or OfflineDataset.
            random_seed (int): A random seed for the random
                number generator. Defaults to 123.
            max_bayes_iter (int): The maximum number of iterations of Bayesian
                optimization.
            bounds (np.ndarray): The bounds for optimization. If None, default
                boundaries for the kernel will be used. Otherwise, must be an
                array of shape (# hyperparams, 2).
            bayes_tol (float): Criteria for convergence for Bayesian
                optimization.
            n_pts_per_dim (int): The number of grid points per shared hparam.
            n_cycles (int): The number of cycles of "telescoping" grid search
                to run. Increasing n_pts_per_dim and n_cycles usually only
                results in very small improvements in performance.
            subsample (float): A value in the range [0.01,1] that indicates what
                fraction of the training set to use each time the score is
                calculated (the same subset is used every time). In general, 1
                will give better results, but using a subsampled subset can be
                a fast way to find the (approximate) location of a good
                hyperparameter set.
            eigval_quotient (float): A value by which the largest singular value
                of Z^T Z is divided to determine the smallest acceptable eigenvalue
                of Z^T Z (singular vectors with eigenvalues smaller than this
                are discarded). Setting this to larger values will make this
                slightly more accurate, at the risk of numerical stability issues.
                In general, do not change this without good reason.
            min_eigval (float): If the largest singular value of Z^T Z divided by
                eigval_quotient is < min_eigval, min_eigval is used as the cutoff
                threshold instead. Setting this to smaller values will make this
                slightly more accurate, at the risk of numerical stability
                issues. In general, do not change this without good reason.

        Returns:
            hyperparams (np.ndarray): The best hyperparams found during optimization.
            n_feval (int): The number of function evaluations during optimization.
            best_score (float): The best negative marginal log-likelihood achieved.
            scores (tuple): A tuple where the first element is the sigma values
                evaluated and the second is the resulting scores. Can be useful
                for diagnostic purposes.

        Raises:
            ValueError: The input dataset is checked for validity before tuning is
                initiated. If problems are found, a raise exception will provide an
                explanation of the error. This method will also raise an exception
                if you try to use it on a kernel with > 4 hyperparameters (since
                this strategy no longer provides any benefit under those conditions)
                or < 3.
            n_init_pts (int): The number of initial grid points to evaluate before
                Bayesian optimization. 10 (the default) is usually fine. If you are
                searcing a smaller space, however, you can save time by using
                a smaller # (e.g. 5).
        """
        bounds = self._run_pretuning_prep(dataset, random_seed, bounds, "exact")
        num_hparams = self.kernel.get_hyperparams().shape[0]
        if num_hparams == 2:
            best_score, hyperparams = shared_hparam_search(np.array([]), self.kernel,
                    dataset, bounds, n_pts_per_dim, n_cycles, subsample = subsample)
            n_feval, scores = 1, ()
        elif 5 > num_hparams > 2:
            hyperparams, scores, best_score, n_feval = bayes_grid_tuning(self.kernel,
                                dataset, bounds, random_seed, max_bayes_iter,
                                self.verbose, bayes_tol,
                                n_pts_per_dim, n_cycles, n_init_pts,
                                subsample = subsample,
                                eigval_quotient = eigval_quotient,
                                min_eigval = min_eigval)

        else:
            raise ValueError("The crude_bayes procedure is only appropriate for "
                    "kernels with 3-4 hyperparameters.")

        self._post_tuning_cleanup(dataset, hyperparams)
        return hyperparams, n_feval, best_score, scores




    def tune_hyperparams_fine_direct(self, dataset, bounds = None,
                    optim_method = "Powell", starting_hyperparams = None,
                    random_seed = 123, max_iter = 50, nmll_rank = 1024,
                    nmll_probes = 25, nmll_iter = 500,
                    nmll_tol = 1e-6, pretransform_dir = None,
                    preconditioner_mode = "srht_2"):
        """Tunes hyperparameters using either Nelder-Mead or Powell (Powell
        preferred), with an approximate NMLL calculation instead of exact.
        This is generally not useful for searching the whole hyperparameter space.
        If given a good starting point, however, it can work pretty well.
        Consequently it is best used to fine-tune "crude" hyperparameters obtained
        from an initial tuning run with a small number of random features in a
        less scalable method (e.g. minimal bayes).

        This method uses approximate NMLL. Note that the approximation will only be
        good so long as the 'nmll' parameters selected are reasonable.

        Args:
            dataset: Object of class OnlineDataset or OfflineDataset.
            bounds (np.ndarray): The bounds for optimization.
                Either a 2d numpy array of shape (num_hyperparams, 2) or None,
                in which case default kernel boundaries are used.
            optim_method (str): One of "Powell", "Nelder-Mead".
            starting_hyperparams (np.ndarray): A starting point for optimization.
                Defaults to None. If None, randomly selected locations are used.
            random_seed (int): Seed for the random number generator.
            max_iter (int): The maximum number of iterations.
            nmll_rank (int): The preconditioner rank for approximate NMLL estimation.
                A larger value may reduce the number of iterations for nmll
                approximation to converge and improve estimation accuracy, but
                will also increase computational cost for preconditioner construction.
            nmll_probes (int): The number of probes for approximate NMLL estimation.
                A larger value may improve accuracy of estimation but with increased
                computational cost.
            nmll_iter (int): The maximum number of iterations for approximate NMLL.
                A larger value may improve accuracy of estimation but with increased
                computational cost.
            nmll_tol (float): The convergence tolerance for approximate NMLL.
                A smaller value may improve accuracy of estimation but with
                increased computational cost.
            pretransform_dir (str): Either None or a valid filepath where pretransformed
                data can be saved. If not None, the dataset is "pretransformed" before
                each round of evaluations when using approximate NMLL. This can take up a
                lot of disk space if the number of random features is large, but can
                greatly increase speed of fitting for convolution kernels with large #
                random features.
            preconditioner_mode (str): One of "srht", "srht_2". Determines the mode of
                preconditioner construction. "srht" is cheaper, requiring one pass over
                the dataset, but lower quality. "srht_2" requires two passes over the
                dataset but is better. Prefer "srht_2" unless you are running on CPU,
                in which case "srht" may be preferable.

        Returns:
            hyperparams (np.ndarray): The best hyperparams found during optimization.
            n_feval (int): The number of function evaluations during optimization.
                best_score (float): The best negative marginal log-likelihood achieved.

        Raises:
            ValueError: The input dataset is checked for validity before tuning is
                initiated. If problems are found, a ValueError will provide an
                explanation of the error.
        """
        if optim_method not in ["Powell", "Nelder-Mead"]:
            raise ValueError("optim_method must be in ['Powell', 'Nelder-Mead']")
        #If the user passed starting hyperparams, use them. Otherwise,
        #if a kernel already exists, use those hyperparameters. Otherwise...
        init_hyperparams = starting_hyperparams
        if init_hyperparams is None and self.kernel is not None:
            init_hyperparams = self.kernel.get_hyperparams()
            init_hyperparams = np.round(init_hyperparams, 1)

        if nmll_rank >= self.training_rffs:
            raise ValueError("NMLL rank must be < the number of training rffs.")
        bounds = self._run_pretuning_prep(dataset, random_seed, bounds, "approximate")
        #If kernel was only just now created by run_pretuning_prep and
        #no starting hyperparams were passed, use the mean of the bounds.
        #This is...not good in general, but running multiple restarts with NM
        #is pretty expensive, which is why user is recommended to specify a
        #starting point for NM. The mean of the bounds is a (bad) default.
        if init_hyperparams is None:
            init_hyperparams = np.mean(bounds, axis=1)

        bounds_tuples = list(map(tuple, bounds))
        if self.verbose:
            print("Now beginning NM minimization.")

        args = (dataset, nmll_rank, nmll_probes, random_seed,
                    nmll_iter, nmll_tol, pretransform_dir,
                    preconditioner_mode)

        if optim_method == "Powell":
            res = minimize(self.approximate_nmll, x0 = init_hyperparams,
                options={"maxfev":max_iter, "xtol":1e-1, "ftol":1},
                method=optim_method, args = args, bounds = bounds_tuples)
        elif optim_method == "Nelder-Mead":
            res = minimize(self.approximate_nmll, x0 = init_hyperparams,
                options={"maxfev":max_iter,
                        "xatol":1e-1, "fatol":1e-1},
                method=optim_method, args = args, bounds = bounds_tuples)

        self._post_tuning_cleanup(dataset, res.x)
        return res.x, res.nfev, res.fun



    def tune_hyperparams_crude_lbfgs(self, dataset, random_seed = 123,
            max_iter = 20, n_restarts = 1, starting_hyperparams = None,
            bounds = None, subsample = 1):
        """Tunes the hyperparameters using the L-BFGS algorithm.
        It uses either a supplied set of starting hyperparameters OR
        randomly chosen locations. If the latter, it is run
        n_restarts times. Because it uses exact NMLL rather than
        approximate, this method is only suitable to small numbers
        of random features; scaling to larger numbers of random
        features is quite poor. Using > 5000 random features in
        this method will be fairly slow. It may also be less useful
        for large datasets. This is therefore intended as a "quick-
        and-dirty" method. Often however the hyperparameters obtained
        using this method are good enough that no further fine-tuning
        is required.

        Args:
            dataset: Object of class OnlineDataset or OfflineDataset.
            random_seed (int): A random seed for the random
                number generator. Defaults to 123.
            max_iter (int): The maximum number of iterations for
                which l-bfgs should be run per restart.
            n_restarts (int): The maximum number of restarts to run l-bfgs.
            starting_hyperparams (np.ndarray): A starting point for l-bfgs
                based optimization. Defaults to None. If None, randomly
                selected locations are used.
            bounds (np.ndarray): The bounds for optimization. Defaults to
                None, in which case the kernel uses its default bounds.
                If supplied, must be a 2d numpy array of shape (num_hyperparams, 2).
            subsample (float): A value in the range [0.01,1] that indicates what
                fraction of the training set to use each time the gradient is
                calculated (the same subset is used every time). In general, 1
                will give better results, but using a subsampled subset can be
                a fast way to find the (approximate) location of a good
                hyperparameter set.

        Returns:
            hyperparams (np.ndarray): The best hyperparams found during optimization.
            n_feval (int): The number of function evaluations during optimization.
            best_score (float): The best negative marginal log-likelihood achieved.

        Raises:
            ValueError: The input dataset is checked for validity before tuning is
                initiated. If problems are found, a ValueError will provide an
                explanation of the error.
        """
        if subsample < 0.01 or subsample > 1:
            raise ValueError("subsample must be in the range [0.01, 1].")

        init_hyperparams = starting_hyperparams
        if init_hyperparams is None and self.kernel is not None:
            init_hyperparams = self.kernel.get_hyperparams(logspace=True)

        bounds = self._run_pretuning_prep(dataset, random_seed, bounds, "exact")
        best_x, best_score, net_iterations = None, np.inf, 0
        bounds_tuples = list(map(tuple, bounds))

        if init_hyperparams is None:
            init_hyperparams = self.kernel.get_hyperparams(logspace=True)

        rng = np.random.default_rng(random_seed)
        args, cost_fun = (dataset, subsample), self.exact_nmll_gradient

        if self.verbose:
            print("Now beginning L-BFGS minimization.")

        for iteration in range(n_restarts):
            res = minimize(cost_fun, options={"maxiter":max_iter},
                        x0 = init_hyperparams, args = args,
                        jac = True, bounds = bounds_tuples)

            net_iterations += res.nfev
            if res.fun < best_score:
                best_x = res.x
                best_score = res.fun
            if self.verbose:
                print(f"Restart {iteration} completed. Best score is {best_score}.")

            init_hyperparams = [rng.uniform(low = bounds[j,0], high = bounds[j,1])
                    for j in range(bounds.shape[0])]
            init_hyperparams = np.asarray(init_hyperparams)

        if best_x is None:
            raise ValueError("All restarts failed to find acceptable hyperparameters.")

        self._post_tuning_cleanup(dataset, best_x)
        return best_x, net_iterations, best_score



    def tune_hyperparams_crude_sgd(self, dataset, random_seed = 123,
            n_epochs = 1, minibatch_size = 1000, learn_rate = 0.02,
            n_restarts = 5, bounds = None,
            start_averaging = 1000, nmll_method = "approx",
            nmll_rank = 1024, nmll_probes = 25, nmll_iter = 200,
            nmll_tol = 1e-6):
        """Tunes the hyperparameters using SGD. Note that this uses
        a biased estimator to the full gradient, so it will almost
        certainly find a suboptimal set of hyperparameters. In spite
        of this, it can be useful for finding a good starting point
        for another method (e.g. Nelder-Mead), because it is fast and
        can quickly search large regions of hyperparameter space.
        Bear in mind however that the solutions it provides may
        be a considerable difference from optimal.

        Args:
            dataset: Object of class OnlineDataset or OfflineDataset.
            random_seed (int): A random seed for the random
                number generator.
            n_epochs: The maximum number of epochs per restart.
            minibatch_size (int): The size of the minibatches.
            learn_rate (float): The initial learning rate.
            n_restarts (int): The maximum number of restarts to run.
                Ignored if starting_hyperparams are supplied.
            bounds (np.ndarray): The bounds for optimization. Defaults to
                None, in which case the kernel uses its default bounds.
                If supplied, must be a 2d numpy array of shape (num_hyperparams, 2).
            start_averaging (int): The epoch number on which to start averaging
                (if performing stochastic hparam averaging or "Polyak-Ruppert"
                averaging). If greater than n_epochs, no averaging is performed.
            nmll_method (str): one of "exact", "approx". Determines how the score
                for a set of hyperparameters at the end of a restart is calculated.
                "exact" is better for small datasets & numbers of rffs. "approx"
                is much slower for small datasets / num rffs but much
                faster for large.
            nmll_rank (int): The preconditioner rank for approximate NMLL estimation.
                Ignored if nmll_method is "exact". A larger value may reduce
                the number of iterations for nmll approximation to converge and
                improve estimation accuracy, but will also increase computational
                cost for preconditioner construction.
            nmll_probes (int): The number of probes for approximate NMLL estimation.
                Ignored if nmll_method is "exact". A larger value may improve
                accuracy of estimation but with increased computational cost.
            nmll_iter (int): The maximum number of iterations for approximate NMLL.
                Ignored if nmll_method is "exact". A larger value may improve
                accuracy of estimation but with increased computational cost.
            nmll_tol (float): The convergence tolerance for approximate NMLL.
                Ignored if nmll_method is "exact". A smaller value may improve
                accuracy of estimation but with increased computational cost.

        Returns:
            hyperparams (np.ndarray): The best hyperparams found during optimization.
            n_feval (int): The number of function evaluations during optimization.
            best_score (float): The best negative marginal log-likelihood achieved.

        Raises:
            ValueError: The input dataset is checked for validity before tuning is
                    initiated. If problems are found, a ValueError will provide an
                    explanation of the error.
        """
        bounds = self._run_pretuning_prep(dataset, random_seed, bounds, nmll_method)

        rng = np.random.default_rng(random_seed)
        best_cost, net_iterations, best_params = np.inf, 0, None
        all_costs = []
        for i in range(n_restarts):
            init_hparams = [rng.uniform(low = bounds[j,0], high = bounds[j,1])
                    for j in range(bounds.shape[0])]
            init_hparams = np.asarray(init_hparams)
            dataset.reset_index()
            #If a non-positive definite matrix is encountered, we've stumbled
            #across a really bad set of hyperparameters. This doesn't mean we
            #need to stop dead in our tracks -- it means we need to move to
            #the next restart and print some kind of notification to user.
            try:
                hyperparams, iterations = amsgrad_optimizer(self.minibatch_nmll_gradient,
                                init_hparams, dataset,
                                bounds, minibatch_size,
                                n_epochs, learn_rate,
                                start_averaging, self.verbose)
                if self.verbose:
                    print(hyperparams)
            except:
                if self.verbose:
                    print("Failure / non-positive definite matrix on stochastic "
                        "optimization attempt. Retrying...")
                continue
            net_iterations += iterations

            if nmll_method == "exact":
                cost = self.exact_nmll(hyperparams[:init_hparams.shape[0]], dataset)
            else:
                cost = self.approximate_nmll(hyperparams[:init_hparams.shape[0]],
                        dataset, nmll_rank, nmll_probes, random_seed,
                        nmll_iter, nmll_tol)
            if cost < best_cost:
                best_cost = copy.copy(cost)
                best_params = copy.deepcopy(hyperparams[:init_hparams.shape[0]])
                if self.verbose:
                    print(f"New best cost: {best_cost}")
            all_costs.append(cost)
            if self.verbose:
                print("\n\n")

        dataset.reset_index()
        if best_params is None:
            raise ValueError("No stochastic optimization attempt succeeded.")
        return best_params, net_iterations, best_cost




    def _run_pretuning_prep(self, input_dataset, random_seed, input_bounds = None,
                        nmll_method = "exact"):
        """Checks the dataset supplied by the user to ensure
        it is consistent with the kernel choice and other user selections.
        Initializes the kernel and pretransforms the data (if appropriate).

        Args:
            dataset: Object of class OnlineDataset or OfflineDataset.
                You should generate this object using either the
                build_online_dataset, build_offline_fixed_vector_dataset
                or build_offline_sequence_dataset functions under
                data_handling.dataset_builder, or a static_layer if applicable.
            random_seed (int): A random seed for the random number generator.
            input_bounds (np.ndarray): The bounds for optimization. Defaults to
                None, in which case the kernel uses its default bounds.
                If supplied, must be a 2d numpy array of shape (num_hyperparams, 2).
            nmll_method (str): "exact" or something else. If exact, only up to
                MAX_CLOSED_FORM_RFFS random features are allowed.

        Returns:
            bounds (np.ndarray): If input_bounds were specified, these are the input_bounds
                specified by the user. If not, they are the kernel's defaults.
        """
        if self.verbose:
            print("starting_tuning")
        if input_dataset.pretransformed:
            raise ValueError("You cannot supply a pretransformed dataset for tuning.")

        input_dataset.device = self.device
        self.kernel = None
        self.weights, self.var = None, None

        if self.device == "gpu":
            mempool = cp.get_default_memory_pool()
            mempool.free_all_blocks()
        self.kernel = self._initialize_kernel(self.kernel_choice, input_dataset.get_xdim(),
                        self.training_rffs, random_seed, input_bounds)

        if input_bounds is None:
            bounds = self.kernel.get_bounds(logspace=True)
        else:
            bounds = input_bounds
        #We check num rffs AFTER initializing the kernel, since one kernel (Linear!)
        #will override requested number of rffs.
        if self.kernel.get_num_rffs() > constants.MAX_CLOSED_FORM_RFFS and nmll_method == "exact":
            raise ValueError(f"At most {constants.MAX_CLOSED_FORM_RFFS} can be used "
                        "for tuning hyperparameters using this method. Try tuning "
                        "using approximate nmll instead.")

        return bounds


    def _post_tuning_cleanup(self, input_dataset, best_hyperparams):
        """Runs some post-tuning cleanup operations
        that are common to all optimization strategies."""
        if self.verbose:
            print("Tuning complete.")
        input_dataset.device = "cpu"
        self.kernel.set_hyperparams(best_hyperparams, logspace=True)
        if self.device == "gpu":
            mempool = cp.get_default_memory_pool()
            mempool.free_all_blocks()


    #############################
    #The next block of functions are used for fitting models once the hyperparameters
    #have been tuned (or, alternatively, fitting using user-supplied hyperparameters).
    #############################


    def pretransform_data(self, input_dataset, pretransform_dir,
            random_seed = 123, preset_hyperparams = None):
        """Pretransforms the data using the kernel (initializing one if
        none present), in other words, pre-generates random features and
        saves them on disk. This is GREATLY preferable if you are fitting
        on CPU, and even on GPU may be much faster for convolution kernels.
        It may however take up a large amount of diskspace if both the
        dataset and the number of random features are very large.

        Args:
            input_dataset: A Dataset object.
            random_seed (int): A random seed for the random number generator.
            hyperparams (np.ndarray): Either None or a user-supplied numpy array
                containing the hyperparameters. It must be valid for the kernel
                in question. If None, the hyperparameters must already have been
                tuned so that self.kernel is not None. If neither of these is True,
                a ValueError will be raised.
            pretransform_dir (str): A valid filepath to a directory.

        Returns:
            output_datset: A Dataset object with the pretransformed training data.
        """
        self._run_fitting_prep(input_dataset, random_seed, preset_hyperparams)
        fitting_dataset = self._pretransform_dataset(input_dataset, pretransform_dir)
        fitting_dataset.device = self.device
        return fitting_dataset


    def _run_fitting_prep(self, input_dataset, random_seed, hyperparams = None):
        """Checks the dataset supplied by the user to ensure
        it is consistent with the kernel choice and other user selections.

        Args:
            dataset: A Dataset object.
            random_seed (int): A random seed for the random number generator.
            hyperparams (np.ndarray): Either None or a user-supplied numpy array
                containing the hyperparameters. It must be valid for the kernel
                in question. If None, the hyperparameters must already have been
                tuned so that self.kernel is not None. If neither of these is True,
                a ValueError will be raised.
        """
        if self.device == "gpu":
            mempool = cp.get_default_memory_pool()
            mempool.free_all_blocks()

        input_dataset.device = self.device

        self.weights, self.var = None, None
        self.trainy_mean = input_dataset.get_ymean()
        self.trainy_std = input_dataset.get_ystd()

        if hyperparams is None:
            if self.kernel is None:
                raise ValueError("In order to fit, either hyperparameters should first "
                        "have been tuned, or user-specified hyperparameters must be "
                        "supplied.")
            starting_hparams = self.kernel.get_hyperparams(logspace = True)
        else:
            starting_hparams = hyperparams
        #It's better to reinitialize the kernel. This may seem wasteful BUT it's
        #actually very cheap for FHT kernels AND if the user has changed anything
        #(kernel_spec_parms,
        #fitting_rffs etc.) since the time that they initialized the model,
        #we'll need to reinitialize the kernel anyway. As long as we've kept
        #the hyperparameters -- that's the important piece. Also, as long
        #as user is using the same random seed, kernel will be the same anyway.
        if input_dataset.pretransformed:
            xdim = input_dataset.parent_xdim
        else:
            xdim = input_dataset.get_xdim()
        self.kernel = self._initialize_kernel(self.kernel_choice,
                        xdim, self.fitting_rffs, random_seed)

        self.kernel.check_hyperparams(starting_hparams)
        self.kernel.set_hyperparams(starting_hparams, logspace = True)



    def fit(self, dataset, preconditioner = None,
                tol = 1e-6, preset_hyperparams=None, max_iter = 500,
                random_seed = 123, run_diagnostics = False,
                mode = "cg", suppress_var = False,
                manual_lr = None):
        """Fits the model after checking that the input data
        is consistent with the kernel choice and other user selections.

        Args:
            dataset: Object of class OnlineDataset or OfflineDataset.
            preconditioner: Either None or a valid Preconditioner (e.g.
                CudaRandomizedPreconditioner, CPURandomizedPreconditioner
                etc). If None, no preconditioning is used.
            tol (float): The threshold below which iterative strategies (L-BFGS, CG,
                SGD) are deemed to have converged. Defaults to 1e-5. Note that how
                reaching the threshold is assessed may depend on the algorithm.
            preset_hyperparams: Either None or a numpy array. If None,
                hyperparameters must already have been tuned using one
                of the tuning methods (e.g. tune_hyperparams_bayes_bfgs).
                If supplied, must be a numpy array of shape (N, 2) where
                N is the number of hyperparams for the kernel in question.
            max_iter (int): The maximum number of epochs for iterative strategies.
            random_seed (int): The random seed for the random number generator.
            run_diagnostics (bool): If True, the number of conjugate
                gradients and the preconditioner diagnostics ratio are returned.
            mode (str): Must be one of "sgd", "amsgrad", "cg", "lbfgs", "exact".
                Determines the approach used. If 'exact', self.kernel.get_num_rffs
                must be <= constants.constants.MAX_CLOSED_FORM_RFFS.
            suppress_var (bool): If True, do not calculate variance. This is generally only
                useful when optimizing hyperparameters, since otherwise we want to calculate
                the variance. It is best to leave this as default False unless performing
                hyperparameter optimization.
            manual_lr (float): Either None or a float. If not None, this is the initial
                learning rate used for stochastic gradient descent or ams grad (ignored
                for all other fitting modes). If None, the algorithm will try to determine
                a good initial learning rate itself.

        Returns:
            Does not return anything unless run_diagnostics is True.
            n_iter (int): The number of iterations for conjugate gradients, L-BFGS or sgd.
            losses (list): The loss on each iteration. Only for SGD and CG, otherwise,
                empty list.

        Raises:
            ValueError: The input dataset is checked for validity before tuning is
                initiated, an error is raised if problems are found."""
        self._run_fitting_prep(dataset, random_seed, preset_hyperparams)
        if self.verbose:
            print("starting fitting")
        n_iter, losses = 0, []
        if mode == "exact":
            if self.kernel.get_num_rffs() > constants.MAX_CLOSED_FORM_RFFS:
                raise ValueError("You specified 'exact' fitting, but self.fitting_rffs is "
                        f"> {constants.MAX_CLOSED_FORM_RFFS}.")
            self.weights = self._calc_weights_exact(dataset)
        elif mode == "cg":
            self.weights, n_iter, losses = self._calc_weights_cg_lib_ext(dataset,
                                        cg_tol = tol, preconditioner = preconditioner,
                                        max_iter = max_iter)
        elif mode == "cg_test":
            self.weights, n_iter, losses = self._calc_weights_cg(dataset,
                                        cg_tol = tol, preconditioner = preconditioner,
                                        max_iter = max_iter)
        elif mode == "lbfgs":
            self.weights, n_iter = self._calc_weights_lbfgs(dataset,
                                        tol = tol, max_iter = max_iter)
        elif mode == "sgd":
            self.weights, n_iter, losses = self._calc_weights_sgd(dataset,
                                        tol = tol, max_iter = max_iter,
                                        preconditioner = preconditioner,
                                        manual_lr = manual_lr)

        elif mode == "amsgrad":
            self.weights, n_iter, losses = self._calc_weights_ams(dataset,
                                        tol = tol, max_iter = max_iter)
        else:
            raise ValueError("Unrecognized fitting mode supplied. Must provide one of "
                        "'lbfgs', 'cg', 'sgd', 'amsgrad', 'exact'.")
        if not suppress_var:
            self.var = self._calc_variance(dataset)

        if self.verbose:
            print("Fitting complete.")
        if self.device == "gpu":
            mempool = cp.get_default_memory_pool()
            mempool.free_all_blocks()
        if run_diagnostics:
            return n_iter, losses


    ####The remaining functions are all getters / setters.


    @property
    def kernel_spec_parms(self):
        """Property definition for the kernel_spec_parms."""
        return self._kernel_spec_parms

    @kernel_spec_parms.setter
    def kernel_spec_parms(self, value):
        """Setter for kernel_spec_parms."""
        if not isinstance(value, dict):
            raise ValueError("Tried to set kernel_spec_parms to something that "
                    "was not a dict!")
        self._kernel_spec_parms = value


    @property
    def kernel_choice(self):
        """Property definition for the kernel_choice attribute."""
        return self._kernel_choice

    @kernel_choice.setter
    def kernel_choice(self, value):
        """Setter for the kernel_choice attribute."""
        if not isinstance(value, str):
            raise ValueError("You supplied a kernel_choice that is not a string.")
        if value not in KERNEL_NAME_TO_CLASS:
            raise ValueError("You supplied an unrecognized kernel.")
        self._kernel_choice = value

    @property
    def fitting_rffs(self):
        """Property definition for the fitting_rffs attribute."""
        return self._fitting_rffs

    @fitting_rffs.setter
    def fitting_rffs(self, value):
        """Setter for the fitting_rffs attribute."""
        self._fitting_rffs = value


    @property
    def training_rffs(self):
        """Property definition for the training_rffs attribute."""
        return self._training_rffs

    @training_rffs.setter
    def training_rffs(self, value):
        """Setter for the training_rffs attribute."""
        self._training_rffs = value

    @property
    def variance_rffs(self):
        """Property definition for the variance_rffs attribute."""
        return self._variance_rffs

    @variance_rffs.setter
    def variance_rffs(self, value):
        """Setter for the variance_rffs attribute."""
        if value > constants.MAX_VARIANCE_RFFS:
            raise ValueError("Currently to keep computational expense at acceptable "
                    f"levels variance rffs is capped at {constants.MAX_VARIANCE_RFFS}.")
        if value > self.fitting_rffs:
            raise ValueError("variance_rffs must be <= fitting_rffs.")
        self._variance_rffs = value


    @property
    def double_precision_fht(self):
        """Property definition for the double_precision_fht attribute."""
        return self._double_precision_fht

    @double_precision_fht.setter
    def double_precision_fht(self, value):
        """Setter for the double_precision_fht attribute."""
        self._double_precision_fht = value

    @property
    def device(self):
        """Property definition for the device attribute."""
        return self._device

    @device.setter
    def device(self, value):
        """Setter for the device attribute."""
        if value not in ["cpu", "gpu"]:
            raise ValueError("Device must be in ['cpu', 'gpu'].")

        if "cupy" not in sys.modules and value == "gpu":
            raise ValueError("You have specified the gpu fit mode but CuPy is "
                "not installed. Currently CPU only fitting is available.")

        if "cuda_basic_hadamard_operations" not in sys.modules and value == "gpu":
            raise ValueError("You have specified the gpu fit mode but the "
                "cudaHadamardTransform module is not installed / "
                "does not appear to have installed correctly. "
                "Currently CPU only fitting is available.")

        if self.kernel is not None:
            self.kernel.device = value
        if self.weights is not None:
            if value == "gpu":
                self.weights = cp.asarray(self.weights)
            elif value == "cpu" and not isinstance(self.weights, np.ndarray):
                self.weights = cp.asnumpy(self.weights)
        if self.var is not None:
            if value == "gpu":
                self.var = cp.asarray(self.var)
            elif value == "cpu" and not isinstance(self.weights, np.ndarray):
                self.weights = cp.asnumpy(self.var)
        if value == "gpu":
            mempool = cp.get_default_memory_pool()
            mempool.free_all_blocks()
        self._device = value
